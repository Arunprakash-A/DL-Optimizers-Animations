# Gradient Based Optimization Algorithms - Visualized
- These are all the animations used in the Optimization lectures taught by Prof.Mitesh Khapra at IIT Madras in the [Deep Learning course](https://cse.iitm.ac.in/~miteshk/CS6910.html).
- I have uploaded the notebook that I used to create these animations (or) you can directly go to colab [here](https://colab.research.google.com/drive/1LEpnHVFpHzuNIFDWS6GCFw4DIKqe2aBu?usp=sharing) 
- That notebook has an implementation for the gradient descent algorithm. You need to modify the update rule for each optimization algorithm. 
- The objective is to get an intuitive idea of the differences in the optimization algorithms with contrived examples.
- You can find all the animations used in the lecture in .mp4 format in the Animations directory
- Here are a few samples

## Gradient Descent
![Alternate image text](animations/GD_2D.gif)

## Momentum Gradient Descent
![Alternate image text](animations/mgd.gif)

## Nesterov Accelarated Gradient Descent
![Alternate image text](animations/nag.gif)

## AdaGrad
![Alternate image text](animations/adagrad.gif)

## Adam
![Alternate image text](animations/nag.gif)
